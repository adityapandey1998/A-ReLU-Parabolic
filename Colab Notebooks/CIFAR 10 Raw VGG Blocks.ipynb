{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Test] Raw VGG Blocks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "msNINEuEV_yh"
      },
      "source": [
        "# import tensorflow as tf \n",
        "# from tensorflow import keras \n",
        "# import numpy as np \n",
        "# import matplotlib.pyplot as plt \n",
        "# import sys \n",
        "\n",
        "# from keras.datasets import cifar10\n",
        "# from keras.models import Sequential \n",
        "# from keras.layers import Dense, Dropout, Flatten \n",
        "# from keras.layers import Conv2D, MaxPooling2D\n",
        "# from keras.optimizers import SGD\n",
        "# from keras.regularizers import l2\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers import BatchNormalization\n",
        "# from time import time \n",
        "\n",
        "# def load_dataset():\n",
        "#     #load dataset\n",
        "#     (trainX, trainY),(testX, testY) = cifar10.load_data()\n",
        "#     #one hot encode the target \n",
        "#     trainY = keras.utils.to_categorical(trainY)\n",
        "#     testY = keras.utils.to_categorical(testY)\n",
        "#     return trainX, trainY, testX, testY\n",
        "\n",
        "# def validation_split(testX, testY, valid_X, valid_Y, v_split):\n",
        "    \n",
        "#     index_of_validation = int(v_split * len(testX))\n",
        "#     valid_X.extend(testX[-index_of_validation:])\n",
        "#     valid_Y.extend(testY[-index_of_validation:])\n",
        "#     testX = testX[:-index_of_validation]\n",
        "#     testY = testY[:-index_of_validation]\n",
        "#     return testX, testY, np.asarray(valid_X), np.asarray(valid_Y)\n",
        "\n",
        "# def normalize(train,test,valid):\n",
        "#     # convert from integers to float \n",
        "#     train_norm = train.astype('float32')\n",
        "#     test_norm = test.astype('float32')\n",
        "#     valid_norm = valid.astype('float32')\n",
        "#     #normalize to range 0-1\n",
        "#     train_norm = train_norm / 255.0\n",
        "#     test_norm = test_norm / 255.0\n",
        "#     valid_norm = valid_norm / 255.0\n",
        "#     return train_norm, test_norm,valid_norm \n",
        "\n",
        "# def define_model():\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv2D(32,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same', input_shape = (32,32,3)))\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(Conv2D(32,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(MaxPooling2D((2,2)))\n",
        "#     model.add(Dropout(0.2))\n",
        "#     model.add(Conv2D(64,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(Conv2D(64,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(MaxPooling2D((2,2)))\n",
        "#     model.add(Dropout(0.3))\n",
        "#     model.add(Conv2D(128,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(Conv2D(128,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(MaxPooling2D((2,2)))\n",
        "#     model.add(Dropout(0.4))\n",
        "   \n",
        "\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
        "#     model.add(BatchNormalization())\n",
        "#     model.add(Dropout(0.5))\n",
        "#     model.add(Dense(10, activation = 'softmax'))\n",
        "    \n",
        "#     #compile model \n",
        "#     opt = SGD(lr = 0.001, momentum = 0.9)\n",
        "#     model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # plot diagnostic learning curves\n",
        "# def summarize_diagnostics(history):\n",
        "#     plt.subplots(figsize = (7,7))\n",
        "#     # plot loss\n",
        "#     plt.subplot(211)\n",
        "#     plt.title('Cross Entropy Loss')\n",
        "#     plt.plot(history.history['loss'], color='blue', label='train')\n",
        "#     plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\n",
        "#     # plot accuracy\n",
        "#     plt.subplot(212)\n",
        "#     plt.title('Classification Accuracy')\n",
        "#     plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "#     plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "#     plt.show()\n",
        "    \n",
        "\n",
        "# # run all the defined functions for evaluating a model\n",
        "# def test_model():\n",
        "#     # load dataset\n",
        "#     trainX, trainY, testX, testY = load_dataset()\n",
        "#     #get validation set \n",
        "#     valid_X = []\n",
        "#     valid_Y = []\n",
        "#     testX, testY, validX, validY = validation_split(testX, testY, valid_X, valid_Y,v_split=0.5)\n",
        "    \n",
        "#     # normalize the data\n",
        "#     trainX, testX,validX = normalize(trainX, testX,validX)\n",
        "#     # define model\n",
        "#     model = define_model()\n",
        "#     #create data generator \n",
        "#     datagen = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True)\n",
        "#     #iterator \n",
        "#     train = datagen.flow(trainX, trainY, batch_size = 64)\n",
        "#     # fit model\n",
        "#     steps = int(trainX.shape[0]/ 64)\n",
        "#     history = model.fit_generator(train, steps_per_epoch = steps, epochs=400, validation_data=(validX, validY), verbose=0)\n",
        "#     # evaluate model\n",
        "#     _, acc = model.evaluate(testX, testY, verbose=0)\n",
        "#     print('> %.3f' % (acc * 100.0))\n",
        "#     # learning curves\n",
        "#     summarize_diagnostics(history)\n",
        "#     return history\n",
        "\n",
        "# def main():\n",
        "# \ttest_model()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKB3fnMgbFme"
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import sys \n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout, Flatten \n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IRkGUq7bSdc"
      },
      "source": [
        "(trainX, trainY),(testX, testY) = cifar10.load_data()\n",
        "trainY = keras.utils.to_categorical(trainY)\n",
        "testY = keras.utils.to_categorical(testY)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQgcPy8yfkd5"
      },
      "source": [
        "valid_X = []\n",
        "valid_Y = []\n",
        "\n",
        "v_split = 0.5\n",
        "\n",
        "index_of_validation = int(v_split * len(testX))\n",
        "valid_X.extend(testX[-index_of_validation:])\n",
        "valid_Y.extend(testY[-index_of_validation:])\n",
        "testX = testX[:-index_of_validation]\n",
        "testY = testY[:-index_of_validation]\n",
        "# return testX, testY, np.asarray(valid_X), np.asarray(valid_Y)\n",
        "validX, validY = np.asarray(valid_X), np.asarray(valid_Y)\n",
        "# testX, testY, validX, validY = validation_split(testX, testY, valid_X, valid_Y,v_split=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_mIX6EwfuSu"
      },
      "source": [
        "#Normalize\n",
        "\n",
        "train_norm = trainX.astype('float32')\n",
        "test_norm = testX.astype('float32')\n",
        "valid_norm = validX.astype('float32')\n",
        "#normalize to range 0-1\n",
        "train_norm = train_norm / 255.0\n",
        "test_norm = test_norm / 255.0\n",
        "valid_norm = valid_norm / 255.0\n",
        "\n",
        "(trainX, testX, validX) = (train_norm, test_norm, valid_norm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCmdOvF6f4YZ"
      },
      "source": [
        "#Modeling\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same', input_shape = (32,32,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(64,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Conv2D(128,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128,(3,3), activation = 'relu', kernel_initializer = 'he_uniform', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#compile model \n",
        "opt = SGD(lr = 0.001, momentum = 0.9)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLVIisR2oMw9"
      },
      "source": [
        "from keras.applications import VGG19\n",
        "base_model_1 = VGG19(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=trainY.shape[1])\n",
        "for layer in base_model_1.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(base_model_1)\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(1024, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
        "model_1.add(Dense(512, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(Dropout(0.5))\n",
        "model_1.add(Dense(128, activation = 'relu', kernel_initializer = 'he_uniform'))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(Dropout(0.5))\n",
        "model_1.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "model_1.summary()\n",
        "\n",
        "#compile model \n",
        "opt = SGD(lr = 0.005, momentum = 0.9)\n",
        "opt = Adam(lr = 0.005)\n",
        "model_1.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd7TZiqSgCSC"
      },
      "source": [
        "# testX, testY, validX, validY = validation_split(testX, testY, valid_X, valid_Y,v_split=0.5)\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True)\n",
        "\n",
        "batch_size = 64\n",
        "train = datagen.flow(trainX, trainY, batch_size = batch_size)\n",
        "steps = int(trainX.shape[0]/ batch_size)\n",
        "history = model_1.fit_generator(train, steps_per_epoch = steps, epochs=150, validation_data=(validX, validY), verbose=1)\n",
        "\n",
        "# _, acc = model_1.evaluate(testX, testY, verbose=0)\n",
        "# print('> %.3f' % (acc * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ4_unwGhMIV"
      },
      "source": [
        "from sklearn import metrics\n",
        "score = model_1.evaluate(testX, testY, verbose=0)\n",
        "y_pred = model_1.predict_classes(testX)\n",
        "y_true=np.argmax(testY,axis=1)\n",
        "# Print test accuracy\n",
        "print('\\n', 'Test accuracy:', round(score[1], 5))\n",
        "print('\\n', 'Test Loss:', round(score[0], 5))\n",
        "print(metrics.classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "\n",
        "plt.subplots(figsize = (7,7))\n",
        "# plot loss\n",
        "plt.subplot(211)\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='train')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\n",
        "# plot accuracy\n",
        "plt.subplot(212)\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CqfRH-3np0M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}